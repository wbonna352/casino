[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Casino",
    "section": "",
    "text": "Projekt symulacji kasyna to projekt portfolio big data, którego celem jest symulacja środowiska kasyna. Prezentuje umiejętności w zakresie systemów rozproszonych, strumieniowania danych i formatów data lake. Projekt wykorzystuje Docker Compose, Apache Kafka, PostgreSQL, Debezium, Spark Structured Streaming oraz MinIO z Apache Iceberg. Projekt jest symlucją migracji danych z bazy do produkcyjnej do środowiska analitycznego. Link do repozytorium: casino."
  },
  {
    "objectID": "index.html#docker-compose",
    "href": "index.html#docker-compose",
    "title": "Casino",
    "section": "Docker Compose",
    "text": "Docker Compose\nProjekt jest konteneryzowany za pomocą Docker Compose dla łatwego uruchamiania i orkiestracji.\n\nKontenery\n\ncasino-postgres: Kontener z bazą danych PostgreSQL, która jest bazą produkcyjną kasyna.\ncasino-zookeeper: Kontener Apache ZooKeeper, używany do zarządzania koordynacją Kafki.\ncasino-kafka: Kontener Apache Kafka, obsługujący przesyłanie strumieniowych danych między usługami.\ncasino-debezium: Kontener Debezium, służący do rejestrowania zmian w bazie danych PostgreSQL i przesyłania ich do Kafki.\ncasino-curl: Kontener z curl, używany do inicjowania połączenia Debezium z PostgreSQL.\ncasino-init-db: Kontener uruchamiający skrypt w Pythonie, który inicjalizuje bazę danych PostgreSQL.\ncasino-minio: Kontener MinIO, przechowujący dane w formacie Iceberg (obiektowe przechowywanie danych).\ncasino-spark-master: Kontener Spark Master, zarządzający zadaniami obliczeniowymi w klastrze Spark.\ncasino-spark-worker: Kontener Spark Worker, wykonujący zadania obliczeniowe rozdzielane przez Spark Master.\ncasino-spark-players: Kontener Spark do przetwarzania strumieniowych danych związanych z graczami.\ncasino-spark-transactions: Kontener Spark do przetwarzania strumieniowych danych związanych z transakcjami.\ncasino-spark-games: Kontener Spark do przetwarzania strumieniowych danych związanych z grami.\ncasino-user1: Kontener Python dla Gracza 1, symulujący interakcję z grą kasynową.\ncasino-user2: Kontener Python dla Gracza 2, symulujący interakcję z grą kasynową.\ncasino-user3: Kontener Python dla Gracza 3, symulujący interakcję z grą kasynową."
  },
  {
    "objectID": "index.html#flow-danych",
    "href": "index.html#flow-danych",
    "title": "Casino",
    "section": "Flow danych",
    "text": "Flow danych\nDane są generowane przez trzech graczy, którzy wysyłają informacje do bazy danych PostgreSQL. Zmiany w bazie danych są rejestrowane przez Debezium, który następnie przesyła je do systemu Kafka. Z Kafka, dane są odczytywane przez Spark i przetwarzane, a na końcu zapisywane do MinIO w formacie Iceberg.\n\n\n\n\n\ngraph TD\n    A[Player 1] --&gt; D[Postgres]\n    B[Player 2] --&gt; D\n    C[Player 3] --&gt; D\n    D --&gt; |Debezium| E[Kafka]\n    E --&gt; |Spark| F[Minio]"
  },
  {
    "objectID": "index.html#baza-produkcyjna",
    "href": "index.html#baza-produkcyjna",
    "title": "Casino",
    "section": "Baza produkcyjna",
    "text": "Baza produkcyjna\nBaza produkcyjna kasyna jest oparta na PostgreSQL i jest tworzona oraz zarządzana za pomocą SQLAlchemy.\n\nplayersgamestransactions\n\n\n\nSELECT *\nFROM players\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\nid\nfirst_name\nlast_name\nemail\naccount_balance\ncreated_at\nupdated_at\n\n\n\n\n3\nEthan\nClark\nethan.clark@example.com\n0.55\n2024-12-13 19:44:21\n2024-12-13 20:37:02\n\n\n1\nOlivia\nBennett\nolivia.bennett@example.com\n0.83\n2024-12-13 19:44:21\n2024-12-13 20:09:59\n\n\n2\nLucas\nHayes\nlucas.hayes@example.com\n53.87\n2024-12-13 19:44:21\n2024-12-13 20:48:19\n\n\n\n\n\n\n\n\nSELECT *\nFROM games\nLIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nid\nplayer_id\ntype\nstake\nresult\npayout\ncreated_at\nupdated_at\n\n\n\n\n1\n3\nrange_roulette\n130.00\nloss\n0\n2024-12-13 19:44:31\nNA\n\n\n2\n1\nstraight_roulette\n40.00\nloss\n0\n2024-12-13 19:44:51\nNA\n\n\n3\n1\nstraight_roulette\n67.20\nloss\n0\n2024-12-13 19:44:55\nNA\n\n\n4\n1\nstraight_roulette\n8.93\nloss\n0\n2024-12-13 19:44:57\nNA\n\n\n5\n1\nstraight_roulette\n53.03\nloss\n0\n2024-12-13 19:45:02\nNA\n\n\n\n\n\n\n\n\nSELECT *\nFROM transactions\nLIMIT 5\n\n\n3 records\n\n\nid\nplayer_id\ntype\nvalue\ncreated_at\nupdated_at\n\n\n\n\n1\n3\ndeposit\n1000\n2024-12-13 19:44:21\nNA\n\n\n2\n1\ndeposit\n1000\n2024-12-13 19:44:21\nNA\n\n\n3\n2\ndeposit\n1000\n2024-12-13 19:44:21\nNA"
  },
  {
    "objectID": "index.html#rodzaje-gier",
    "href": "index.html#rodzaje-gier",
    "title": "Casino",
    "section": "Rodzaje gier",
    "text": "Rodzaje gier\n\nStraight Roulette (Ruletka Prosta)\n\nZasady: Gracz stawia zakład na konkretny numer na kole ruletki (numery od 0 do 36).\nMechanizm: Po zakręceniu kołem, losowany jest numer.\n\nWygrana: Gracz wygrywa, jeśli wylosowany numer to 0 (zero).\nPrzegrana: W przeciwnym przypadku gracz przegrywa.\n\nWypłata: Jeśli gracz wygra (numer = 0), wypłata wynosi 36 razy stawka. W przypadku przegranej wypłata wynosi 0.\n\n\n\nRange Roulette (Ruletka Zakresowa)\n\nZasady: Gracz stawia zakład na to, czy wylosowany numer na kole ruletki będzie większy niż 18.\nMechanizm: Po zakręceniu kołem, losowany jest numer.\n\nWygrana: Gracz wygrywa, jeśli wylosowany numer jest większy niż 18 (czyli w zakresie od 19 do 36).\nPrzegrana: Gracz przegrywa, jeśli numer jest w zakresie od 0 do 18.\n\nWypłata: Jeśli gracz wygra (numer &gt; 18), wypłata wynosi 2 razy stawka. W przypadku przegranej wypłata wynosi 0.\n\n\n\nW obu przypadkach:\n\nWynik: Określany jest przez losowanie numeru i porównanie go z odpowiednią wartością lub zakresem.\nPayout (wypłata): Kwota, którą gracz otrzymuje, jeśli wygra, lub 0, jeśli przegra."
  },
  {
    "objectID": "index.html#strategie-graczy",
    "href": "index.html#strategie-graczy",
    "title": "Casino",
    "section": "Strategie Graczy",
    "text": "Strategie Graczy\n\n\n\n\n\n\nNote\n\n\n\nDane graczy użyte w tym projekcie są fikcyjne i zostały stworzone wyłącznie w celach demonstracyjnych.\n\n\n\nEthan Clark\n\nEthan gra w Ruletkę Zakresową (Range Roulette).\nCzas oczekiwania przed każdym zakładem wynosi losowo od 1 do 60 sekund.\nStawka jest obliczana na podstawie salda konta gracza, gdzie stawka to losowa wartość w przedziale od 5% do 50% salda konta, z gwarancją, że stawka nie będzie mniejsza niż 1.\n\n\n\nOlivia Bennett\n\nOlivia gra w Prostą Ruletkę (Straight Roulette).\nCzas oczekiwania przed zakładem wynosi losowo od 1 do 15 sekund.\nStawka jest obliczana na podstawie salda konta gracza, gdzie stawka to losowa wartość w przedziale od 1% do 10% salda konta, z gwarancją, że stawka nie będzie mniejsza niż 1.\n\n\n\nLucas Hayes\n\nLucas gra w jedną z dwóch gier: Prostą Ruletkę (Straight Roulette) lub Ruletkę Zakresową (Range Roulette).\nCzas oczekiwania przed zakładem wynosi losowo od 10 do 60 sekund.\nWybór gry jest losowy. Dla każdej z gier:\n\nJeśli gra w Prostą Ruletkę (Straight Roulette), stawka to losowa wartość w przedziale od 1% do 5% salda konta, z gwarancją, że stawka nie będzie mniejsza niż 1.\nJeśli gra w Ruletkę Zakresową (Range Roulette), stawka to losowa wartość w przedziale od 5% do 50% salda konta, z gwarancją, że stawka nie będzie mniejsza niż 1."
  },
  {
    "objectID": "index.html#struktura-bucketa-minio",
    "href": "index.html#struktura-bucketa-minio",
    "title": "Casino",
    "section": "Struktura Bucketa MinIO",
    "text": "Struktura Bucketa MinIO\nDane kasyna oraz metadane związane z przepływem danych są przechowywane w buckecie MinIO w strukturze przedstawionej poniżej:\ncasino/\n├── checkpoint/\n│   ├── players/\n│   ├── games/\n│   └── transactions/\n├── warehouse/\n│   └── default/\n│       ├── players/\n│       │   ├── data/\n│       │   └── metadata/\n│       ├── games/\n│       │   ├── data/\n│       │   └── metadata/\n│       └── transactions/\n│           ├── data/\n│           └── metadata/\nDane kasyna są najpierw przesyłane do Apache Kafka, gdzie są przechowywane i przetwarzane. Następnie Spark odczytuje te dane z Kafka i przetwarza je. Przetworzone dane są zapisywane do MinIO w formacie Iceberg w katalogu warehouse.\nKatalog checkpoint zawiera dane i metadane związane z etapami przetwarzania danych w Apache Spark. Spark używa katalogu checkpoint, aby przechowywać dane pośrednie, które mogą zostać wykorzystane w przypadku wznowienia procesu przetwarzania.\nKatalog warehouse zawiera finalne dane oraz metadane. Są to dane pochodzące z tabel players, games i transactions, które zostały przetworzone przez Spark i zapisane do MinIO w formacie Iceberg."
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Casino Report",
    "section": "",
    "text": ":: loading settings :: url = jar:file:/home/wbonna/repos/github/casino/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml"
  },
  {
    "objectID": "report.html#historia-stanu-konta-gracza",
    "href": "report.html#historia-stanu-konta-gracza",
    "title": "Casino Report",
    "section": "Historia stanu konta gracza",
    "text": "Historia stanu konta gracza\n\nOlivia BennettLucas HayesEthan Clark"
  },
  {
    "objectID": "report.html#podsumowanie",
    "href": "report.html#podsumowanie",
    "title": "Casino Report",
    "section": "Podsumowanie",
    "text": "Podsumowanie\n\n\n\n\n\n\n\n\n\nname\n\n\n\ndeposit\n\n\n\nstakes\n\n\n\npayouts\n\n\n\ngames_win\n\n\n\ngames_loss\n\n\n\n\n\n\n\n\n\n\n\nOlivia Bennett\n\n\n\n1000.0\n\n\n\n1604.22\n\n\n\n605.02\n\n\n\n5\n\n\n\n180\n\n\n\n\n\n\n\nLucas Hayes\n\n\n\n1000.0\n\n\n\n2934.12\n\n\n\n1988.05\n\n\n\n19\n\n\n\n76\n\n\n\n\n\n\n\nEthan Clark\n\n\n\n1000.0\n\n\n\n6643.25\n\n\n\n5643.76\n\n\n\n53\n\n\n\n54"
  },
  {
    "objectID": "src/usun.html",
    "href": "src/usun.html",
    "title": "casino",
    "section": "",
    "text": "from casino_spark.main import default_spark\nfrom casino_spark.functions import bigint_to_timestamp\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\nfrom datetime import datetime\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nspark = default_spark()\n\n24/12/13 17:31:00 WARN Utils: Your hostname, ubuntu-main resolves to a loopback address: 127.0.1.1; using 192.168.33.5 instead (on interface enp0s31f6)\n24/12/13 17:31:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nIvy Default Cache set to: /home/wbonna/.ivy2/cache\nThe jars for the packages stored in: /home/wbonna/.ivy2/jars\n\n\n:: loading settings :: url = jar:file:/home/wbonna/repos/github/casino/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n\n\norg.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\norg.apache.hadoop#hadoop-aws added as a dependency\norg.apache.hadoop#hadoop-client-api added as a dependency\norg.apache.hadoop#hadoop-client-runtime added as a dependency\ncom.amazonaws#aws-java-sdk-bundle added as a dependency\norg.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-aa493dc9-8030-4a97-a330-22d67d4e088b;1.0\n    confs: [default]\n    found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n    found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n    found org.apache.kafka#kafka-clients;3.4.1 in central\n    found org.lz4#lz4-java;1.8.0 in central\n    found org.xerial.snappy#snappy-java;1.1.10.3 in central\n    found org.slf4j#slf4j-api;2.0.7 in central\n    found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n    found org.apache.hadoop#hadoop-client-api;3.3.4 in central\n    found commons-logging#commons-logging;1.1.3 in central\n    found com.google.code.findbugs#jsr305;3.0.0 in central\n    found org.apache.commons#commons-pool2;2.11.1 in central\n    found org.apache.hadoop#hadoop-aws;3.3.4 in central\n    found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n    found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n    found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 in central\n:: resolution report :: resolve 401ms :: artifacts dl 14ms\n    :: modules in use:\n    com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n    com.google.code.findbugs#jsr305;3.0.0 from central in [default]\n    commons-logging#commons-logging;1.1.3 from central in [default]\n    org.apache.commons#commons-pool2;2.11.1 from central in [default]\n    org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n    org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n    org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n    org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 from central in [default]\n    org.apache.kafka#kafka-clients;3.4.1 from central in [default]\n    org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n    org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n    org.lz4#lz4-java;1.8.0 from central in [default]\n    org.slf4j#slf4j-api;2.0.7 from central in [default]\n    org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n    org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-aa493dc9-8030-4a97-a330-22d67d4e088b\n    confs: [default]\n    0 artifacts copied, 15 already retrieved (0kB/10ms)\n24/12/13 17:31:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n\nnow = datetime.now()\nfor table in [\"players\", \"transactions\", \"games\"]:\n    print(spark.table(table).filter(F.col(\"timestamp\") &lt; now).count())\n\n24/12/13 17:31:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n                                                                                \n\n\n118\n3\n112\n\n\n\nfor table in [\"players\", \"games\", \"transactions\"]:\n    print(table)\n    spark.table(table).select(F.count(\"*\"), F.countDistinct(\"offset\")).show()\n\nplayers\n\n\n                                                                                \n\n\n+--------+----------------------+\n|count(1)|count(DISTINCT offset)|\n+--------+----------------------+\n|     142|                   142|\n+--------+----------------------+\n\ngames\n+--------+----------------------+\n|count(1)|count(DISTINCT offset)|\n+--------+----------------------+\n|     136|                   136|\n+--------+----------------------+\n\ntransactions\n+--------+----------------------+\n|count(1)|count(DISTINCT offset)|\n+--------+----------------------+\n|       3|                     3|\n+--------+----------------------+\n\n\n\n\nspark.sql(\"\"\"\nSELECT value.*\nFROM players\nWHERE value.updated_at is null\n\"\"\").toPandas()\n\n\n\n\n\n\n\n\nid\nfirst_name\nlast_name\nemail\naccount_balance\ncreated_at\nupdated_at\n\n\n\n\n0\n1\nOlivia\nBennett\nolivia.bennett@example.com\n0.0\n1734110280844981\nNaN\n\n\n1\n2\nLucas\nHayes\nlucas.hayes@example.com\n0.0\n1734110280939854\nNaN\n\n\n2\n3\nEthan\nClark\nethan.clark@example.com\n0.0\n1734110280953703\nNaN\n\n\n\n\n\n\n\n\nspark.read.table(\"games\").count()\n\n\n(\n    spark.table(\"players\")\n    .select(F.min(\"offset\"))\n    .collect()\n    ,\n    spark.table(\"players\")\n    .select(F.min(\"offset\"), F.sum(\"offset\"))\n    .collect()\n)\n\n\nplayers = spark.table(\"players\")\nplayers.cache()\n\n\n(\n    spark.table(\"players\")\n    .select(F.min(\"value.payload.after.created_at\"))\n    .collect()\n)\n\n\n(\n    spark.table(\"transactions\")\n    .withColumn(\"x\", F.col(\"value.payload.after.created_at\").cast(\"int\"))\n    .select(F.min(\"x\"))\n    .collect()\n)\n\n\nspark.table(\"players\").select(F.min(\"offset\")).show()\n\nplayers = spark.table(\"players\")\nplayers.cache()\n\nplayers.select(F.min(\"offset\")).show()\n\n\nspark.sql(\"SELECT * FROM default.players.history order by made_current_at limit 1\").toPandas()\n\n\nspark.sql(\"select * from default.players version as of 4789673466297209583 order by offset\").toPandas()\n\n\nspark.table(\"players\").select(F.min(\"offset\"), F.sum(\"offset\")).explain(True)\n\n\nspark.table(\"players\").orderBy(\"offset\").select(F.min(\"offset\")).show()\n\n\nspark.table(\"games\").select(F.min(\"offset\"), F.sum(\"offset\")).collect()\n\n\n(\n    spark.table(\"transactions\")\n    .select(\"offset\", \"timestamp\", \"value.payload.after.*\")\n    .withColumn(\"created_at\", bigint_to_timestamp(F.col(\"created_at\")))\n    .orderBy(\"offset\")\n).toPandas()\n\n\nspark.sql(\"\"\"\nCALL system.rewrite_data_files(\n    table =&gt; 'default.games',\n    options =&gt; map('target-file-size-bytes', '536870912')\n)\n\"\"\")\n\n\nspark.sql(\"\"\"\nCALL system.expire_snapshots('default.games')\n\"\"\")\n\n\nspark.sql(\"\"\"\nCALL system.remove_orphan_files('default.games')\n\"\"\")\n\n\nspark.sql(\"\"\"\nCALL spark_catalog.system.rewrite_manifests(\n    table =&gt; 'default.games'\n)\n\"\"\")\n\n\nspark.table(\"games\").select(F.min(\"offset\")).collect()\n\n\nspark.table(\"players\").select(F.min(\"offset\")).collect()"
  }
]