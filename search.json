[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "casino",
    "section": "",
    "text": "Schemat projektu Casino\n\n\n\n\n\ngraph TD\n  player1[Player 1] --&gt;|Wygeneruj dane| postgres[Postgres]\n  player2[Player 2] --&gt;|Wygeneruj dane| postgres\n  player3[Player 3] --&gt;|Wygeneruj dane| postgres\n  postgres --&gt;|Nasluchiwanie zmian| debezium[Debezium]\n  debezium --&gt;|Zapis| kafka[Kafka]\n  kafka --&gt;|Czytanie wiadomości| spark_master[Spark Master]\n  spark_master --&gt;|Przetwarzanie| spark_worker[Spark Worker]\n  spark_worker --&gt;|Zapis| minio[Minio]\n  zookeeper[Zookeeper] --&gt;|Zarządzanie| kafka"
  },
  {
    "objectID": "src/db/usun.html",
    "href": "src/db/usun.html",
    "title": "casino",
    "section": "",
    "text": "from models import Player\nfrom base import session\n\n\nplayer: Player = Player.query.filter(Player.email == \"example@example.com\").first()\n\n\nsession.rollback()\n\n\nplayer.make_deposit(1000)\n\n\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\n\nspark = SparkSession.builder \\\n    .appName(\"notebook\") \\\n    .config(\"spark.jars.packages\",\n            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n            \"org.apache.hadoop:hadoop-client-api:3.3.4,\"\n            \"org.apache.hadoop:hadoop-client-runtime:3.3.4,\"\n            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"s3a://casino/warehouse\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", \"s3a://casino/checkpoint/\") \\\n    .config(\"spark.sql.catalog.spark_catalog.uri\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n    .getOrCreate()\n\n24/12/06 23:32:07 WARN Utils: Your hostname, ubuntu-main resolves to a loopback address: 127.0.1.1; using 192.168.8.124 instead (on interface wlp0s20f3)\n24/12/06 23:32:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nIvy Default Cache set to: /home/wbonna/.ivy2/cache\nThe jars for the packages stored in: /home/wbonna/.ivy2/jars\norg.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\norg.apache.hadoop#hadoop-aws added as a dependency\norg.apache.hadoop#hadoop-client-api added as a dependency\norg.apache.hadoop#hadoop-client-runtime added as a dependency\ncom.amazonaws#aws-java-sdk-bundle added as a dependency\norg.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-7b9c26f0-f73d-4f3e-b76d-2fa1f24729fb;1.0\n    confs: [default]\n\n\n:: loading settings :: url = jar:file:/home/wbonna/repos/casino/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n\n\n    found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n    found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n    found org.apache.kafka#kafka-clients;3.4.1 in central\n    found org.lz4#lz4-java;1.8.0 in central\n    found org.xerial.snappy#snappy-java;1.1.10.3 in central\n    found org.slf4j#slf4j-api;2.0.7 in central\n    found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n    found org.apache.hadoop#hadoop-client-api;3.3.4 in central\n    found commons-logging#commons-logging;1.1.3 in central\n    found com.google.code.findbugs#jsr305;3.0.0 in central\n    found org.apache.commons#commons-pool2;2.11.1 in central\n    found org.apache.hadoop#hadoop-aws;3.3.4 in central\n    found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n    found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n    found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 in central\ndownloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.4.0/iceberg-spark-runtime-3.5_2.12-1.4.0.jar ...\n    [SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0!iceberg-spark-runtime-3.5_2.12.jar (20372ms)\n:: resolution report :: resolve 996ms :: artifacts dl 20381ms\n    :: modules in use:\n    com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n    com.google.code.findbugs#jsr305;3.0.0 from central in [default]\n    commons-logging#commons-logging;1.1.3 from central in [default]\n    org.apache.commons#commons-pool2;2.11.1 from central in [default]\n    org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n    org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n    org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n    org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 from central in [default]\n    org.apache.kafka#kafka-clients;3.4.1 from central in [default]\n    org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n    org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n    org.lz4#lz4-java;1.8.0 from central in [default]\n    org.slf4j#slf4j-api;2.0.7 from central in [default]\n    org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n    org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   15  |   1   |   1   |   0   ||   15  |   1   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-7b9c26f0-f73d-4f3e-b76d-2fa1f24729fb\n    confs: [default]\n    1 artifacts copied, 14 already retrieved (28423kB/20ms)\n24/12/06 23:32:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/12/06 23:32:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n24/12/06 23:32:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n\n\n\nplayers = spark.read.table(\"players\")\n\n\nplayers \\\n    .select(F.max(\"timestamp\")) \\\n    .show(truncate=False)\n\n+-----------------------+\n|max(timestamp)         |\n+-----------------------+\n|2024-12-06 16:11:39.327|\n+-----------------------+\n\n\n\n\ntables = spark.sql(\"show tables\").collect()\n\n\n[t.tableName for t in tables]\n\n['players']\n\n\n\nspark.sql(\"\"\"\ncall spark_catalog.system.rewrite_data_files('default.players')\n\"\"\")\n\n                                                                                \n\n\nDataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]\n\n\n\nspark.sql(\"\"\"\ncall spark_catalog.system.expire_snapshots(table =&gt; 'default.players', retain_last =&gt; 1)\n\"\"\")\n\n                                                                                \n\n\nDataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]\n\n\n\nspark.sql(\"\"\"\ncall spark_catalog.system.remove_orphan_files(\n    table =&gt; 'default.players'\n)\n\"\"\")\n\nDataFrame[orphan_file_location: string]\n\n\n\nspark.sql(\"\"\"\nSELECT * FROM default.players.files\n\"\"\").toPandas()\n\n\n\n\n\n\n\n\ncontent\nfile_path\nfile_format\nspec_id\nrecord_count\nfile_size_in_bytes\ncolumn_sizes\nvalue_counts\nnull_value_counts\nnan_value_counts\nlower_bounds\nupper_bounds\nkey_metadata\nsplit_offsets\nequality_ids\nsort_order_id\nreadable_metrics\n\n\n\n\n0\n0\ns3a://casino/warehouse/default/players/data/00...\nPARQUET\n0\n1\n3696\n{1: 166, 2: 839, 3: 63, 4: 42, 5: 46, 6: 46, 7...\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1}\n{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n{}\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\nNone\n[4]\nNone\n0\n((166, 1, 0, None, {\"schema\":{\"type, {\"schema\"...\n\n\n1\n0\ns3a://casino/warehouse/default/players/data/00...\nPARQUET\n0\n1\n3696\n{1: 166, 2: 839, 3: 63, 4: 42, 5: 46, 6: 46, 7...\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1}\n{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n{}\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\nNone\n[4]\nNone\n0\n((166, 1, 0, None, {\"schema\":{\"type, {\"schema\"...\n\n\n2\n0\ns3a://casino/warehouse/default/players/data/00...\nPARQUET\n0\n1\n3689\n{1: 166, 2: 832, 3: 63, 4: 42, 5: 46, 6: 46, 7...\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1}\n{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n{}\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\nNone\n[4]\nNone\n0\n((166, 1, 0, None, {\"schema\":{\"type, {\"schema\"...\n\n\n3\n0\ns3a://casino/warehouse/default/players/data/00...\nPARQUET\n0\n1\n3694\n{1: 166, 2: 837, 3: 63, 4: 42, 5: 46, 6: 46, 7...\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1}\n{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n{}\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\nNone\n[4]\nNone\n0\n((166, 1, 0, None, {\"schema\":{\"type, {\"schema\"...\n\n\n4\n0\ns3a://casino/warehouse/default/players/data/00...\nPARQUET\n0\n1\n3697\n{1: 166, 2: 840, 3: 63, 4: 42, 5: 46, 6: 46, 7...\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1}\n{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n{}\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\nNone\n[4]\nNone\n0\n((166, 1, 0, None, {\"schema\":{\"type, {\"schema\"...\n\n\n5\n0\ns3a://casino/warehouse/default/players/data/00...\nPARQUET\n0\n273\n18153\n{1: 195, 2: 13026, 3: 95, 4: 74, 5: 567, 6: 15...\n{1: 273, 2: 273, 3: 273, 4: 273, 5: 273, 6: 27...\n{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n{}\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\n{1: [123, 34, 115, 99, 104, 101, 109, 97, 34, ...\nNone\n[4]\nNone\n0\n((195, 273, 0, None, {\"schema\":{\"type, {\"schem...\n\n\n\n\n\n\n\n\nspark.sql(\"\"\"\ncall spark_catalog.system.expire_snapshots(\ntable =&gt; 'default.players',\nolder_than =&gt; TIMESTAMP '2024-12-06 19:00:00'\n)\n\"\"\")\n\n                                                                                \n\n\nDataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]\n\n\n\n!pip install pyiceberg\n\nCollecting pyiceberg\n  Downloading pyiceberg-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting cachetools&lt;6.0.0,&gt;=5.5.0 (from pyiceberg)\n  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\nCollecting click&lt;9.0.0,&gt;=7.1.1 (from pyiceberg)\n  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\nCollecting fsspec&gt;=2023.1.0 (from pyiceberg)\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting mmh3&lt;6.0.0,&gt;=4.0.0 (from pyiceberg)\n  Downloading mmh3-5.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nCollecting pydantic!=2.4.0,!=2.4.1,&lt;3.0,&gt;=2.0 (from pyiceberg)\n  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\nCollecting pyparsing&lt;4.0.0,&gt;=3.1.0 (from pyiceberg)\n  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.20.0 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from pyiceberg) (2.32.3)\nCollecting rich&lt;14.0.0,&gt;=10.11.0 (from pyiceberg)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nCollecting sortedcontainers==2.4.0 (from pyiceberg)\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\nCollecting strictyaml&lt;2.0.0,&gt;=1.7.0 (from pyiceberg)\n  Downloading strictyaml-1.7.3-py3-none-any.whl.metadata (11 kB)\nCollecting tenacity&lt;10.0.0,&gt;=8.2.3 (from pyiceberg)\n  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\nCollecting annotated-types&gt;=0.6.0 (from pydantic!=2.4.0,!=2.4.1,&lt;3.0,&gt;=2.0-&gt;pyiceberg)\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.27.1 (from pydantic!=2.4.0,!=2.4.1,&lt;3.0,&gt;=2.0-&gt;pyiceberg)\n  Downloading pydantic_core-2.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: typing-extensions&gt;=4.12.2 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from pydantic!=2.4.0,!=2.4.1,&lt;3.0,&gt;=2.0-&gt;pyiceberg) (4.12.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from requests&lt;3.0.0,&gt;=2.20.0-&gt;pyiceberg) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from requests&lt;3.0.0,&gt;=2.20.0-&gt;pyiceberg) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from requests&lt;3.0.0,&gt;=2.20.0-&gt;pyiceberg) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from requests&lt;3.0.0,&gt;=2.20.0-&gt;pyiceberg) (2024.8.30)\nCollecting markdown-it-py&gt;=2.2.0 (from rich&lt;14.0.0,&gt;=10.11.0-&gt;pyiceberg)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from rich&lt;14.0.0,&gt;=10.11.0-&gt;pyiceberg) (2.18.0)\nRequirement already satisfied: python-dateutil&gt;=2.6.0 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from strictyaml&lt;2.0.0,&gt;=1.7.0-&gt;pyiceberg) (2.9.0.post0)\nCollecting mdurl~=0.1 (from markdown-it-py&gt;=2.2.0-&gt;rich&lt;14.0.0,&gt;=10.11.0-&gt;pyiceberg)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: six&gt;=1.5 in /home/wbonna/repos/casino/.venv/lib/python3.11/site-packages (from python-dateutil&gt;=2.6.0-&gt;strictyaml&lt;2.0.0,&gt;=1.7.0-&gt;pyiceberg) (1.16.0)\nDownloading pyiceberg-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 1.4 MB/s eta 0:00:00a 0:00:01\nDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\nDownloading click-8.1.7-py3-none-any.whl (97 kB)\nDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\nDownloading mmh3-5.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\nDownloading pydantic-2.10.3-py3-none-any.whl (456 kB)\nDownloading pydantic_core-2.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 1.7 MB/s eta 0:00:0000:0100:01\nDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\nDownloading rich-13.9.4-py3-none-any.whl (242 kB)\nDownloading strictyaml-1.7.3-py3-none-any.whl (123 kB)\nDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: sortedcontainers, tenacity, pyparsing, pydantic-core, mmh3, mdurl, fsspec, click, cachetools, annotated-types, strictyaml, pydantic, markdown-it-py, rich, pyiceberg\nSuccessfully installed annotated-types-0.7.0 cachetools-5.5.0 click-8.1.7 fsspec-2024.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.0.1 pydantic-2.10.3 pydantic-core-2.27.1 pyiceberg-0.8.1 pyparsing-3.2.0 rich-13.9.4 sortedcontainers-2.4.0 strictyaml-1.7.3 tenacity-9.0.0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]